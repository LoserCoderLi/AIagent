{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain.vectorstores import FAISS\n",
    "# 设置LLM\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "# 引入上下文压缩\n",
    "from langchain.retrievers import ContextualCompressionRetriever # 上下文压缩检索\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.react.agent import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.language_models.base import BaseLanguageModel\n",
    "import time\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.callbacks import CallbackManagerForChainRun\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.utils.input import get_color_mapping\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Tongyi模型\n",
    "api_key = os.getenv(\"KEY_TONGYI\")\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key=api_key,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重写了AgentExecutor\n",
    "class LJHAgentExecutor(AgentExecutor):\n",
    "  \n",
    "  llm: BaseLanguageModel\n",
    "  \n",
    "  def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        # Construct a mapping of tool name to tool for easy lookup\n",
    "        name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "        # We construct a mapping from each tool to a color, used for logging.\n",
    "        color_mapping = get_color_mapping(\n",
    "            [tool.name for tool in self.tools], excluded_colors=[\"green\", \"red\"]\n",
    "        )\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "        # Let's start tracking the number of iterations and time elapsed\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # We now enter the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            next_step_output = self._take_next_step(\n",
    "                name_to_tool_map,\n",
    "                color_mapping,\n",
    "                inputs,\n",
    "                intermediate_steps,\n",
    "                run_manager=run_manager,\n",
    "            )\n",
    "            # print(\"刚刚进入循环:\", next_step_output)\n",
    "            # print(\"inputs:\", inputs)\n",
    "            \n",
    "            if isinstance(next_step_output, AgentFinish):\n",
    "                # print('if isinstance(next_step_output, AgentFinish):', next_step_output)\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "\n",
    "            intermediate_steps.extend(next_step_output)\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                # print(\"if len(next_step_output) == 1:\",next_step_action)\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                # print(\"22222222222222222222222\", tool_return)\n",
    "                \n",
    "                if tool_return is not None:\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            \n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        # 改了\n",
    "        # output = self.agent.return_stopped_response(\n",
    "        #     self.early_stopping_method, intermediate_steps, **inputs\n",
    "        # )\n",
    "        \n",
    "        # print(\"最后输出的内容:\", output)\n",
    "        ai_response = self.llm.invoke(inputs['input'])\n",
    "        # print(\"ai_response\",ai_response.content)\n",
    "        # output['output'] = ai_response.content\n",
    "        final_output = AgentFinish(\n",
    "            return_values={\"output\": ai_response.content},\n",
    "            log=''\n",
    "        )\n",
    "        # print(output)\n",
    "        # print(final_output)\n",
    "        return self._return(final_output, intermediate_steps, run_manager=run_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDoc:\n",
    "    def __init__(self, faiss_db_path, system):\n",
    "        self.faiss_db_path = faiss_db_path\n",
    "        self.template = [\n",
    "            (\"system\",\n",
    "            \"你是一个精通C语言的C语言专家，在这个阶段你的主要目标是根据输入的内容匹配MISRA C2012标准的相关信息，输入内容{question}通常包含着违反的具体规则和错误代码,{context}通常包含着最相近的MISRA C2012细则，你需要根据输入的规则查阅MISRA C2012规范, 确定违反的规范细则是否准确，并将准确的细则和输入的代码一并返回\"),\n",
    "            (\"human\", \"我会给你部分C语言代码和分析结果以及我认为它具体违反了MISRA C2012的哪些规范，你来帮我检查并纠正\\n\"),\n",
    "            (\"ai\", \"没问题!\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "        self.prompt = ChatPromptTemplate.from_messages(self.template)\n",
    "\n",
    "    # 加载向量数据库\n",
    "    def load_vector_db(self):\n",
    "        # 加载已经保存的 FAISS 索引\n",
    "        hf = HuggingFaceEmbeddings(\n",
    "            model_name=\"E:\\\\AAAAWork\\\\python\\\\models\\\\EMB\\\\bce-embedding-base_v1\",  # 替换为实际的模型路径\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "        db = FAISS.load_local(self.faiss_db_path, hf, allow_dangerous_deserialization=True)\n",
    "        return db\n",
    "\n",
    "    # 提问并找到相关文本块\n",
    "    def askAndFindFiles(self, question):\n",
    "        db = self.load_vector_db()  # 加载已有的向量数据库\n",
    "        print(\"db:\",db)\n",
    "        # 采用上下文压缩的方式\n",
    "        retriever = db.as_retriever()\n",
    "        compressor = LLMChainExtractor.from_llm(llm=llm)\n",
    "        compressor_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=retriever\n",
    "        )\n",
    "        return compressor_retriever.get_relevant_documents(query=question)\n",
    "\n",
    "    # 用自然语言和文档聊天\n",
    "    def chatWithDoc(self, question):\n",
    "        _context = \"\"\n",
    "        context = self.askAndFindFiles(question)\n",
    "        for i in context:\n",
    "            _context += i.page_content\n",
    "        message = self.prompt.format_messages(context=_context, question=question)\n",
    "        print(\"chatWithDoc:\",message)\n",
    "        return llm.invoke(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def content_info_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    当需要了解MISRA C2012标准的相关信息，你可以调用此方法，\n",
    "    你调用此工具需要输入信息必须包含违反的MISRA C 2012规则编码 , 违规内容的文字描述 以及 被检测的代码，\n",
    "    文字描述应该包含你想到的可能违反的MISRA C 2012的规范细则，\n",
    "    被检测代码应包含上下至少各三行代码以及相关变量的定义，而不是只有一行，\n",
    "    规则编码 ， 违规内容的文字描述 ， 被检测的代码 三者缺一不可！\n",
    "    然后返回MISRA C2012中对应的规范内容！\n",
    "    \"\"\"\n",
    "    print(\"调用了自定的tool\",query)\n",
    "    query += \"它违背了预制文档向量化数据库里面提到的哪一点？中文回答\"\n",
    "    content = contents_search(query)\n",
    "    \n",
    "    return content\n",
    "      \n",
    "# content_info_tool工具会使用的函数\n",
    "# 读取以及存储的文本向量回答问题\n",
    "def contents_search(question):\n",
    "  contents_chat = ChatDoc(\"./contents.index\", \"你是一个查看被提供的C语言是不是符合C2012标准的秘书。如果不符合标准，你会返回是不符合你参考文本中的哪一条标准,只需要返回对应的标号就行。你会根据下面提供的上下文内容来继续回答问题.\")\n",
    "  response = contents_chat.chatWithDoc(question)\n",
    "  \n",
    "  return response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\"], llm)  \n",
    "\n",
    "# 添加自定义工具到工具列表中\n",
    "tools.append(content_info_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_api_key = os.getenv(\"KEY_SMITH\")\n",
    "\n",
    "# prompt = hub.pull(\"ljh/prompt_create_react_agent\", api_key=hub_api_key)\n",
    "# Prompt 模板\n",
    "template = '''尽力用中文回答下面的问题，你可以使用下面的工具:\n",
    "\n",
    "{tools}\n",
    "\n",
    "使用下面的格式:\n",
    "\n",
    "Question: 你必须回答的问题\n",
    "Thought: 你应该经常考虑该做什么，如果使用工具无法获得准确的结果，你应该终止操作，返回\"陛下，臣妾做不到啊\"这句话\n",
    "Action: 要采取的行动，应该是[{tool_names}]中的一个\n",
    "Action Input: 动作的输入\n",
    "Observation: 行动的结果\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: 我知道最终的答案了\n",
    "Final Answer: 原始输入问题的最终答案\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "agent = create_react_agent(\n",
    "  llm=llm,\n",
    "  tools=tools,\n",
    "  prompt=prompt,\n",
    ")\n",
    "agent_executor = LJHAgentExecutor(\n",
    "  llm=llm,\n",
    "  agent=agent, \n",
    "  tools=tools, \n",
    "  handle_parsing_errors=True,\n",
    "  max_iterations=3 # 最多迭代3次\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    /* 这是一个注释开始\n",
    "    /* 这是嵌套的注释 */\n",
    "    printf(\"This is a test.\\n\");\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "s=agent_executor._call({\"input\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '给定的C语言代码片段是一个简单的程序，包含了一些基本的C语言元素：\\n\\n```c\\n#include <stdio.h>\\n```\\n\\n这行代码包含了`stdio.h`头文件，它是用于标准输入输出的库，提供了像`printf`这样的函数。\\n\\n```c\\nint main() {\\n    /* 这是一个注释开始\\n    /* 这是嵌套的注释 */\\n    printf(\"This is a test.\\\\n\");\\n    return 0;\\n}\\n```\\n\\n- `int main()`：这是主函数的声明，所有C程序的执行都是从这里开始的。\\n- `printf(\"This is a test.\\\\n\");`：这是一个使用`printf`函数的例子，用于输出字符串\"This is a test.\"到控制台，并且在字符串末尾添加了一个换行符`\\\\n`。\\n- 注释：代码中使用了两种注释形式。单行注释以`/*`开始并以`*/`结束，而多行注释则使用`/*`开始和`*/`结束。多行注释内部可以嵌套，即一个注释块内可以包含另一个注释块。\\n\\n总的来说，这段代码遵循了C语言的基本语法和编程实践，但是否符合MISRA C 2012标准取决于具体的规则集，特别是与内存管理、指针使用、错误处理等相关的规则。如果没有具体的违规规则，那么这段代码在标准的C语言实践中是正确的。'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
