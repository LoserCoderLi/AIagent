{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.react.agent import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Tongyi和smith的api\n",
    "api_key = os.getenv(\"KEY_TONGYI\")\n",
    "hub_api_key = os.getenv(\"KEY_SMITH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "# 初始化大模型和模板\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key=api_key,\n",
    "    temperature=0\n",
    ")\n",
    "prompt = hub.pull(\"ljh/prompt_create_react_agent\", api_key=hub_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], metadata={'lc_hub_owner': 'ljh', 'lc_hub_repo': 'prompt_create_react_agent', 'lc_hub_commit_hash': 'c1014c7e52fc1d6d04a9329ab0d78c51081e40fc249d897ce8552e4aec872b3a'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义tool\n",
    "@tool\n",
    "def create_react_agent_demo(query: str)->str:\n",
    "  '''\n",
    "  用于搜索的时候使用的工具\n",
    "  '''\n",
    "  leaders_info = {\n",
    "        \"美国总统\": {\"name\": \"乔·拜登\", \"age\": 81},\n",
    "        \"英国首相\": {\"name\": \"里希·苏纳克\", \"age\": 44}\n",
    "    }\n",
    "  if \"美国总统\" in query:\n",
    "      return f\"美国总统是{leaders_info['美国总统']['name']}，年龄是{leaders_info['美国总统']['age']}岁。\"\n",
    "  elif \"英国首相\" in query:\n",
    "      return f\"英国首相是{leaders_info['英国首相']['name']}，年龄是{leaders_info['英国首相']['age']}岁。\"\n",
    "  else:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 你好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Chain that takes in an input and produces an action and action input.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterator,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import yaml\n",
    "from langchain_core._api import deprecated\n",
    "from langchain_core.agents import AgentAction, AgentFinish, AgentStep\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    BaseCallbackManager,\n",
    "    CallbackManagerForChainRun,\n",
    "    CallbackManagerForToolRun,\n",
    "    Callbacks,\n",
    ")\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableConfig, ensure_config\n",
    "from langchain_core.runnables.utils import AddableDict\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.input import get_color_mapping\n",
    "\n",
    "from langchain.agents.agent_iterator import AgentExecutorIterator\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.tools import InvalidTool\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.utilities.asyncio import asyncio_timeout\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BaseSingleActionAgent(BaseModel):\n",
    "    \"\"\"Base Single Action Agent class.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def return_values(self) -> List[str]:\n",
    "        \"\"\"Return values of the agent.\"\"\"\n",
    "        return [\"output\"]\n",
    "\n",
    "    def get_allowed_tools(self) -> Optional[List[str]]:\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Async given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "\n",
    "    def return_stopped_response(\n",
    "        self,\n",
    "        early_stopping_method: str,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        **kwargs: Any,\n",
    "    ) -> AgentFinish:\n",
    "        \"\"\"Return response when agent has been stopped due to max iterations.\n",
    "\n",
    "        Args:\n",
    "            early_stopping_method: Method to use for early stopping.\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            AgentFinish: Agent finish object.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `early_stopping_method` is not supported.\n",
    "        \"\"\"\n",
    "        if early_stopping_method == \"force\":\n",
    "            # `force` just returns a constant string\n",
    "            return AgentFinish(\n",
    "                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseSingleActionAgent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "        Args:\n",
    "            llm: Language model to use.\n",
    "            tools: Tools to use.\n",
    "            callback_manager: Callback manager to use.\n",
    "            kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            BaseSingleActionAgent: Agent object.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def _agent_type(self) -> str:\n",
    "        \"\"\"Return Identifier of an agent type.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def dict(self, **kwargs: Any) -> Dict:\n",
    "        \"\"\"Return dictionary representation of agent.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Dictionary representation of agent.\n",
    "        \"\"\"\n",
    "        _dict = super().dict()\n",
    "        try:\n",
    "            _type = self._agent_type\n",
    "        except NotImplementedError:\n",
    "            _type = None\n",
    "        if isinstance(_type, AgentType):\n",
    "            _dict[\"_type\"] = str(_type.value)\n",
    "        elif _type is not None:\n",
    "            _dict[\"_type\"] = _type\n",
    "        return _dict\n",
    "\n",
    "    def save(self, file_path: Union[Path, str]) -> None:\n",
    "        \"\"\"Save the agent.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to file to save the agent to.\n",
    "\n",
    "        Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            # If working with agent executor\n",
    "            agent.agent.save(file_path=\"path/agent.yaml\")\n",
    "        \"\"\"\n",
    "        # Convert file to Path object.\n",
    "        if isinstance(file_path, str):\n",
    "            save_path = Path(file_path)\n",
    "        else:\n",
    "            save_path = file_path\n",
    "\n",
    "        directory_path = save_path.parent\n",
    "        directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Fetch dictionary to save\n",
    "        agent_dict = self.dict()\n",
    "        if \"_type\" not in agent_dict:\n",
    "            raise NotImplementedError(f\"Agent {self} does not support saving\")\n",
    "\n",
    "        if save_path.suffix == \".json\":\n",
    "            with open(file_path, \"w\") as f:\n",
    "                json.dump(agent_dict, f, indent=4)\n",
    "        elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n",
    "            with open(file_path, \"w\") as f:\n",
    "                yaml.dump(agent_dict, f, default_flow_style=False)\n",
    "        else:\n",
    "            raise ValueError(f\"{save_path} must be json or yaml\")\n",
    "\n",
    "    def tool_run_logging_kwargs(self) -> Dict:\n",
    "        \"\"\"Return logging kwargs for tool run.\"\"\"\n",
    "        return {}\n",
    "\n",
    "\n",
    "class BaseMultiActionAgent(BaseModel):\n",
    "    \"\"\"Base Multi Action Agent class.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def return_values(self) -> List[str]:\n",
    "        \"\"\"Return values of the agent.\"\"\"\n",
    "        return [\"output\"]\n",
    "\n",
    "    def get_allowed_tools(self) -> Optional[List[str]]:\n",
    "        \"\"\"Get allowed tools.\n",
    "\n",
    "        Returns:\n",
    "            Optional[List[str]]: Allowed tools.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[List[AgentAction], AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with the observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Actions specifying what tool to use.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[List[AgentAction], AgentFinish]:\n",
    "        \"\"\"Async given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with the observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Actions specifying what tool to use.\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "\n",
    "    def return_stopped_response(\n",
    "        self,\n",
    "        early_stopping_method: str,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        **kwargs: Any,\n",
    "    ) -> AgentFinish:\n",
    "        \"\"\"Return response when agent has been stopped due to max iterations.\n",
    "\n",
    "        Args:\n",
    "            early_stopping_method: Method to use for early stopping.\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            AgentFinish: Agent finish object.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `early_stopping_method` is not supported.\n",
    "        \"\"\"\n",
    "        if early_stopping_method == \"force\":\n",
    "            # `force` just returns a constant string\n",
    "            return AgentFinish({\"output\": \"Agent stopped due to max iterations.\"}, \"\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def _agent_type(self) -> str:\n",
    "        \"\"\"Return Identifier of an agent type.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def dict(self, **kwargs: Any) -> Dict:\n",
    "        \"\"\"Return dictionary representation of agent.\"\"\"\n",
    "        _dict = super().dict()\n",
    "        try:\n",
    "            _dict[\"_type\"] = str(self._agent_type)\n",
    "        except NotImplementedError:\n",
    "            pass\n",
    "        return _dict\n",
    "\n",
    "    def save(self, file_path: Union[Path, str]) -> None:\n",
    "        \"\"\"Save the agent.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to file to save the agent to.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If agent does not support saving.\n",
    "            ValueError: If file_path is not json or yaml.\n",
    "\n",
    "        Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            # If working with agent executor\n",
    "            agent.agent.save(file_path=\"path/agent.yaml\")\n",
    "        \"\"\"\n",
    "        # Convert file to Path object.\n",
    "        if isinstance(file_path, str):\n",
    "            save_path = Path(file_path)\n",
    "        else:\n",
    "            save_path = file_path\n",
    "\n",
    "        # Fetch dictionary to save\n",
    "        agent_dict = self.dict()\n",
    "        if \"_type\" not in agent_dict:\n",
    "            raise NotImplementedError(f\"Agent {self} does not support saving.\")\n",
    "\n",
    "        directory_path = save_path.parent\n",
    "        directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if save_path.suffix == \".json\":\n",
    "            with open(file_path, \"w\") as f:\n",
    "                json.dump(agent_dict, f, indent=4)\n",
    "        elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n",
    "            with open(file_path, \"w\") as f:\n",
    "                yaml.dump(agent_dict, f, default_flow_style=False)\n",
    "        else:\n",
    "            raise ValueError(f\"{save_path} must be json or yaml\")\n",
    "\n",
    "    def tool_run_logging_kwargs(self) -> Dict:\n",
    "        \"\"\"Return logging kwargs for tool run.\"\"\"\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "class AgentOutputParser(BaseOutputParser[Union[AgentAction, AgentFinish]]):\n",
    "    \"\"\"Base class for parsing agent output into agent action/finish.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Parse text into agent action/finish.\"\"\"\n",
    "\n",
    "\n",
    "class MultiActionAgentOutputParser(\n",
    "    BaseOutputParser[Union[List[AgentAction], AgentFinish]]\n",
    "):\n",
    "    \"\"\"Base class for parsing agent output into agent actions/finish.\n",
    "\n",
    "    This is used for agents that can return multiple actions.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, text: str) -> Union[List[AgentAction], AgentFinish]:\n",
    "        \"\"\"Parse text into agent actions/finish.\n",
    "\n",
    "        Args:\n",
    "            text: Text to parse.\n",
    "\n",
    "        Returns:\n",
    "            Union[List[AgentAction], AgentFinish]:\n",
    "                List of agent actions or agent finish.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class RunnableAgent(BaseSingleActionAgent):\n",
    "    \"\"\"Agent powered by Runnables.\"\"\"\n",
    "\n",
    "    runnable: Runnable[dict, Union[AgentAction, AgentFinish]]\n",
    "    \"\"\"Runnable to call to get agent action.\"\"\"\n",
    "    input_keys_arg: List[str] = []\n",
    "    return_keys_arg: List[str] = []\n",
    "    stream_runnable: bool = True\n",
    "    \"\"\"Whether to stream from the runnable or not.\n",
    "\n",
    "    If True then underlying LLM is invoked in a streaming fashion to make it possible\n",
    "        to get access to the individual LLM tokens when using stream_log with the Agent\n",
    "        Executor. If False then LLM is invoked in a non-streaming fashion and\n",
    "        individual LLM tokens will not be available in stream_log.\n",
    "    \"\"\"\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def return_values(self) -> List[str]:\n",
    "        \"\"\"Return values of the agent.\"\"\"\n",
    "        return self.return_keys_arg\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\"\"\"\n",
    "        return self.input_keys_arg\n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Based on past history and current inputs, decide what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with the observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\n",
    "        final_output: Any = None\n",
    "        if self.stream_runnable:\n",
    "            # Use streaming to make sure that the underlying LLM is invoked in a\n",
    "            # streaming\n",
    "            # fashion to make it possible to get access to the individual LLM tokens\n",
    "            # when using stream_log with the Agent Executor.\n",
    "            # Because the response from the plan is not a generator, we need to\n",
    "            # accumulate the output into final output and return that.\n",
    "            for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\n",
    "                if final_output is None:\n",
    "                    final_output = chunk\n",
    "                else:\n",
    "                    final_output += chunk\n",
    "        else:\n",
    "            final_output = self.runnable.invoke(inputs, config={\"callbacks\": callbacks})\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[\n",
    "        AgentAction,\n",
    "        AgentFinish,\n",
    "    ]:\n",
    "        \"\"\"Async based on past history and current inputs, decide what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\n",
    "        final_output: Any = None\n",
    "        if self.stream_runnable:\n",
    "            # Use streaming to make sure that the underlying LLM is invoked in a\n",
    "            # streaming\n",
    "            # fashion to make it possible to get access to the individual LLM tokens\n",
    "            # when using stream_log with the Agent Executor.\n",
    "            # Because the response from the plan is not a generator, we need to\n",
    "            # accumulate the output into final output and return that.\n",
    "            async for chunk in self.runnable.astream(\n",
    "                inputs, config={\"callbacks\": callbacks}\n",
    "            ):\n",
    "                if final_output is None:\n",
    "                    final_output = chunk\n",
    "                else:\n",
    "                    final_output += chunk\n",
    "        else:\n",
    "            final_output = await self.runnable.ainvoke(\n",
    "                inputs, config={\"callbacks\": callbacks}\n",
    "            )\n",
    "        return final_output\n",
    "\n",
    "\n",
    "class RunnableMultiActionAgent(BaseMultiActionAgent):\n",
    "    \"\"\"Agent powered by Runnables.\"\"\"\n",
    "\n",
    "    runnable: Runnable[dict, Union[List[AgentAction], AgentFinish]]\n",
    "    \"\"\"Runnable to call to get agent actions.\"\"\"\n",
    "    input_keys_arg: List[str] = []\n",
    "    return_keys_arg: List[str] = []\n",
    "    stream_runnable: bool = True\n",
    "    \"\"\"Whether to stream from the runnable or not.\n",
    "\n",
    "    If True then underlying LLM is invoked in a streaming fashion to make it possible\n",
    "        to get access to the individual LLM tokens when using stream_log with the Agent\n",
    "        Executor. If False then LLM is invoked in a non-streaming fashion and\n",
    "        individual LLM tokens will not be available in stream_log.\n",
    "    \"\"\"\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def return_values(self) -> List[str]:\n",
    "        \"\"\"Return values of the agent.\"\"\"\n",
    "        return self.return_keys_arg\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\n",
    "\n",
    "        Returns:\n",
    "            List of input keys.\n",
    "        \"\"\"\n",
    "        return self.input_keys_arg\n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[\n",
    "        List[AgentAction],\n",
    "        AgentFinish,\n",
    "    ]:\n",
    "        \"\"\"Based on past history and current inputs, decide what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with the observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\n",
    "        final_output: Any = None\n",
    "        if self.stream_runnable:\n",
    "            # Use streaming to make sure that the underlying LLM is invoked in a\n",
    "            # streaming\n",
    "            # fashion to make it possible to get access to the individual LLM tokens\n",
    "            # when using stream_log with the Agent Executor.\n",
    "            # Because the response from the plan is not a generator, we need to\n",
    "            # accumulate the output into final output and return that.\n",
    "            for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\n",
    "                if final_output is None:\n",
    "                    final_output = chunk\n",
    "                else:\n",
    "                    final_output += chunk\n",
    "        else:\n",
    "            final_output = self.runnable.invoke(inputs, config={\"callbacks\": callbacks})\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[\n",
    "        List[AgentAction],\n",
    "        AgentFinish,\n",
    "    ]:\n",
    "        \"\"\"Async based on past history and current inputs, decide what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\n",
    "        final_output: Any = None\n",
    "        if self.stream_runnable:\n",
    "            # Use streaming to make sure that the underlying LLM is invoked in a\n",
    "            # streaming\n",
    "            # fashion to make it possible to get access to the individual LLM tokens\n",
    "            # when using stream_log with the Agent Executor.\n",
    "            # Because the response from the plan is not a generator, we need to\n",
    "            # accumulate the output into final output and return that.\n",
    "            async for chunk in self.runnable.astream(\n",
    "                inputs, config={\"callbacks\": callbacks}\n",
    "            ):\n",
    "                if final_output is None:\n",
    "                    final_output = chunk\n",
    "                else:\n",
    "                    final_output += chunk\n",
    "        else:\n",
    "            final_output = await self.runnable.ainvoke(\n",
    "                inputs, config={\"callbacks\": callbacks}\n",
    "            )\n",
    "\n",
    "        return final_output\n",
    "\n",
    "\n",
    "@deprecated(\n",
    "    \"0.1.0\",\n",
    "    alternative=(\n",
    "        \"Use new agent constructor methods like create_react_agent, create_json_agent, \"\n",
    "        \"create_structured_chat_agent, etc.\"\n",
    "    ),\n",
    "    removal=\"1.0\",\n",
    ")\n",
    "class LLMSingleActionAgent(BaseSingleActionAgent):\n",
    "    \"\"\"Base class for single action agents.\"\"\"\n",
    "\n",
    "    llm_chain: LLMChain\n",
    "    \"\"\"LLMChain to use for agent.\"\"\"\n",
    "    output_parser: AgentOutputParser\n",
    "    \"\"\"Output parser to use for agent.\"\"\"\n",
    "    stop: List[str]\n",
    "    \"\"\"List of strings to stop on.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\n",
    "\n",
    "        Returns:\n",
    "            List of input keys.\n",
    "        \"\"\"\n",
    "        return list(set(self.llm_chain.input_keys) - {\"intermediate_steps\"})\n",
    "\n",
    "    def dict(self, **kwargs: Any) -> Dict:\n",
    "        \"\"\"Return dictionary representation of agent.\"\"\"\n",
    "        _dict = super().dict()\n",
    "        del _dict[\"output_parser\"]\n",
    "        return _dict\n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with the observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        output = self.llm_chain.run(\n",
    "            intermediate_steps=intermediate_steps,\n",
    "            stop=self.stop,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return self.output_parser.parse(output)\n",
    "\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Async given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        output = await self.llm_chain.arun(\n",
    "            intermediate_steps=intermediate_steps,\n",
    "            stop=self.stop,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return self.output_parser.parse(output)\n",
    "\n",
    "    def tool_run_logging_kwargs(self) -> Dict:\n",
    "        \"\"\"Return logging kwargs for tool run.\"\"\"\n",
    "        return {\n",
    "            \"llm_prefix\": \"\",\n",
    "            \"observation_prefix\": \"\" if len(self.stop) == 0 else self.stop[0],\n",
    "        }\n",
    "\n",
    "\n",
    "@deprecated(\n",
    "    \"0.1.0\",\n",
    "    alternative=(\n",
    "        \"Use new agent constructor methods like create_react_agent, create_json_agent, \"\n",
    "        \"create_structured_chat_agent, etc.\"\n",
    "    ),\n",
    "    removal=\"1.0\",\n",
    ")\n",
    "class Agent(BaseSingleActionAgent):\n",
    "    \"\"\"Agent that calls the language model and deciding the action.\n",
    "\n",
    "    This is driven by a LLMChain. The prompt in the LLMChain MUST include\n",
    "    a variable called \"agent_scratchpad\" where the agent can put its\n",
    "    intermediary work.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_chain: LLMChain\n",
    "    \"\"\"LLMChain to use for agent.\"\"\"\n",
    "    output_parser: AgentOutputParser\n",
    "    \"\"\"Output parser to use for agent.\"\"\"\n",
    "    allowed_tools: Optional[List[str]] = None\n",
    "    \"\"\"Allowed tools for the agent. If None, all tools are allowed.\"\"\"\n",
    "\n",
    "    def dict(self, **kwargs: Any) -> Dict:\n",
    "        \"\"\"Return dictionary representation of agent.\"\"\"\n",
    "        _dict = super().dict()\n",
    "        del _dict[\"output_parser\"]\n",
    "        return _dict\n",
    "\n",
    "    def get_allowed_tools(self) -> Optional[List[str]]:\n",
    "        \"\"\"Get allowed tools.\"\"\"\n",
    "        return self.allowed_tools\n",
    "\n",
    "    @property\n",
    "    def return_values(self) -> List[str]:\n",
    "        \"\"\"Return values of the agent.\"\"\"\n",
    "        return [\"output\"]\n",
    "\n",
    "    def _fix_text(self, text: str) -> str:\n",
    "        \"\"\"Fix the text.\n",
    "\n",
    "        Args:\n",
    "            text: Text to fix.\n",
    "\n",
    "        Returns:\n",
    "            str: Fixed text.\n",
    "        \"\"\"\n",
    "        raise ValueError(\"fix_text not implemented for this agent.\")\n",
    "\n",
    "    @property\n",
    "    def _stop(self) -> List[str]:\n",
    "        return [\n",
    "            f\"\\n{self.observation_prefix.rstrip()}\",\n",
    "            f\"\\n\\t{self.observation_prefix.rstrip()}\",\n",
    "        ]\n",
    "\n",
    "    def _construct_scratchpad(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]]\n",
    "    ) -> Union[str, List[BaseMessage]]:\n",
    "        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\n{self.observation_prefix}{observation}\\n{self.llm_prefix}\"\n",
    "        return thoughts\n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\n",
    "        full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
    "        return self.output_parser.parse(full_output)\n",
    "\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Async given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            callbacks: Callbacks to run.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\n",
    "        full_output = await self.llm_chain.apredict(callbacks=callbacks, **full_inputs)\n",
    "        agent_output = await self.output_parser.aparse(full_output)\n",
    "        return agent_output\n",
    "\n",
    "    def get_full_inputs(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create the full inputs for the LLMChain from intermediate steps.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Full inputs for the LLMChain.\n",
    "        \"\"\"\n",
    "        thoughts = self._construct_scratchpad(intermediate_steps)\n",
    "        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\n",
    "        full_inputs = {**kwargs, **new_inputs}\n",
    "        return full_inputs\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return list(set(self.llm_chain.input_keys) - {\"agent_scratchpad\"})\n",
    "\n",
    "    @root_validator(pre=False, skip_on_failure=True)\n",
    "    def validate_prompt(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that prompt matches format.\n",
    "\n",
    "        Args:\n",
    "            values: Values to validate.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Validated values.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `agent_scratchpad` is not in prompt.input_variables\n",
    "             and prompt is not a FewShotPromptTemplate or a PromptTemplate.\n",
    "        \"\"\"\n",
    "        prompt = values[\"llm_chain\"].prompt\n",
    "        if \"agent_scratchpad\" not in prompt.input_variables:\n",
    "            logger.warning(\n",
    "                \"`agent_scratchpad` should be a variable in prompt.input_variables.\"\n",
    "                \" Did not find it, so adding it at the end.\"\n",
    "            )\n",
    "            prompt.input_variables.append(\"agent_scratchpad\")\n",
    "            if isinstance(prompt, PromptTemplate):\n",
    "                prompt.template += \"\\n{agent_scratchpad}\"\n",
    "            elif isinstance(prompt, FewShotPromptTemplate):\n",
    "                prompt.suffix += \"\\n{agent_scratchpad}\"\n",
    "            else:\n",
    "                raise ValueError(f\"Got unexpected prompt type {type(prompt)}\")\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def observation_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the observation with.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def llm_prefix(self) -> str:\n",
    "        \"\"\"Prefix to append the LLM call with.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\n",
    "        \"\"\"Create a prompt for this class.\n",
    "\n",
    "        Args:\n",
    "            tools: Tools to use.\n",
    "\n",
    "        Returns:\n",
    "            BasePromptTemplate: Prompt template.\n",
    "        \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n",
    "        \"\"\"Validate that appropriate tools are passed in.\n",
    "\n",
    "        Args:\n",
    "            tools: Tools to use.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\n",
    "        \"\"\"Get default output parser for this class.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        output_parser: Optional[AgentOutputParser] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Agent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\n",
    "\n",
    "        Args:\n",
    "            llm: Language model to use.\n",
    "            tools: Tools to use.\n",
    "            callback_manager: Callback manager to use.\n",
    "            output_parser: Output parser to use.\n",
    "            kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            Agent: Agent object.\n",
    "        \"\"\"\n",
    "        cls._validate_tools(tools)\n",
    "        llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=cls.create_prompt(tools),\n",
    "            callback_manager=callback_manager,\n",
    "        )\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        _output_parser = output_parser or cls._get_default_output_parser()\n",
    "        return cls(\n",
    "            llm_chain=llm_chain,\n",
    "            allowed_tools=tool_names,\n",
    "            output_parser=_output_parser,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def return_stopped_response(\n",
    "        self,\n",
    "        early_stopping_method: str,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        **kwargs: Any,\n",
    "    ) -> AgentFinish:\n",
    "        \"\"\"Return response when agent has been stopped due to max iterations.\n",
    "\n",
    "        Args:\n",
    "            early_stopping_method: Method to use for early stopping.\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations.\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            AgentFinish: Agent finish object.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `early_stopping_method` is not in ['force', 'generate'].\n",
    "        \"\"\"\n",
    "        if early_stopping_method == \"force\":\n",
    "            # `force` just returns a constant string\n",
    "            return AgentFinish(\n",
    "                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\n",
    "            )\n",
    "        elif early_stopping_method == \"generate\":\n",
    "            # Generate does one final forward pass\n",
    "            thoughts = \"\"\n",
    "            for action, observation in intermediate_steps:\n",
    "                thoughts += action.log\n",
    "                thoughts += (\n",
    "                    f\"\\n{self.observation_prefix}{observation}\\n{self.llm_prefix}\"\n",
    "                )\n",
    "            # Adding to the previous steps, we now tell the LLM to make a final pred\n",
    "            thoughts += (\n",
    "                \"\\n\\nI now need to return a final answer based on the previous steps:\"\n",
    "            )\n",
    "            new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\n",
    "            full_inputs = {**kwargs, **new_inputs}\n",
    "            full_output = self.llm_chain.predict(**full_inputs)\n",
    "            # We try to extract a final answer\n",
    "            parsed_output = self.output_parser.parse(full_output)\n",
    "            if isinstance(parsed_output, AgentFinish):\n",
    "                # If we can extract, we send the correct stuff\n",
    "                return parsed_output\n",
    "            else:\n",
    "                # If we can extract, but the tool is not the final tool,\n",
    "                # we just return the full output\n",
    "                return AgentFinish({\"output\": full_output}, full_output)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"early_stopping_method should be one of `force` or `generate`, \"\n",
    "                f\"got {early_stopping_method}\"\n",
    "            )\n",
    "\n",
    "    def tool_run_logging_kwargs(self) -> Dict:\n",
    "        \"\"\"Return logging kwargs for tool run.\"\"\"\n",
    "        return {\n",
    "            \"llm_prefix\": self.llm_prefix,\n",
    "            \"observation_prefix\": self.observation_prefix,\n",
    "        }\n",
    "\n",
    "\n",
    "class ExceptionTool(BaseTool):\n",
    "    \"\"\"Tool that just returns the query.\"\"\"\n",
    "\n",
    "    name: str = \"_Exception\"\n",
    "    \"\"\"Name of the tool.\"\"\"\n",
    "    description: str = \"Exception tool\"\n",
    "    \"\"\"Description of the tool.\"\"\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        return query\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        return query\n",
    "\n",
    "\n",
    "NextStepOutput = List[Union[AgentFinish, AgentAction, AgentStep]]\n",
    "\n",
    "\n",
    "class AgentExecutor(Chain):\n",
    "    \"\"\"Agent that is using tools.\"\"\"\n",
    "    # llm: BaseLanguageModel\n",
    "\n",
    "    agent: Union[BaseSingleActionAgent, BaseMultiActionAgent]\n",
    "    \"\"\"The agent to run for creating a plan and determining actions\n",
    "    to take at each step of the execution loop.\"\"\"\n",
    "    tools: Sequence[BaseTool]\n",
    "    \"\"\"The valid tools the agent can call.\"\"\"\n",
    "    return_intermediate_steps: bool = False\n",
    "    \"\"\"Whether to return the agent's trajectory of intermediate steps\n",
    "    at the end in addition to the final output.\"\"\"\n",
    "    max_iterations: Optional[int] = 15\n",
    "    \"\"\"The maximum number of steps to take before ending the execution\n",
    "    loop.\n",
    "\n",
    "    Setting to 'None' could lead to an infinite loop.\"\"\"\n",
    "    max_execution_time: Optional[float] = None\n",
    "    \"\"\"The maximum amount of wall clock time to spend in the execution\n",
    "    loop.\n",
    "    \"\"\"\n",
    "    early_stopping_method: str = \"force\"\n",
    "    \"\"\"The method to use for early stopping if the agent never\n",
    "    returns `AgentFinish`. Either 'force' or 'generate'.\n",
    "\n",
    "    `\"force\"` returns a string saying that it stopped because it met a\n",
    "        time or iteration limit.\n",
    "\n",
    "    `\"generate\"` calls the agent's LLM Chain one final time to generate\n",
    "        a final answer based on the previous steps.\n",
    "    \"\"\"\n",
    "    handle_parsing_errors: Union[bool, str, Callable[[OutputParserException], str]] = (\n",
    "        False\n",
    "    )\n",
    "    \"\"\"How to handle errors raised by the agent's output parser.\n",
    "    Defaults to `False`, which raises the error.\n",
    "    If `true`, the error will be sent back to the LLM as an observation.\n",
    "    If a string, the string itself will be sent to the LLM as an observation.\n",
    "    If a callable function, the function will be called with the exception\n",
    "     as an argument, and the result of that function will be passed to the agent\n",
    "      as an observation.\n",
    "    \"\"\"\n",
    "    trim_intermediate_steps: Union[\n",
    "        int, Callable[[List[Tuple[AgentAction, str]]], List[Tuple[AgentAction, str]]]\n",
    "    ] = -1\n",
    "    \"\"\"How to trim the intermediate steps before returning them.\n",
    "    Defaults to -1, which means no trimming.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_agent_and_tools(\n",
    "        cls,\n",
    "        agent: Union[BaseSingleActionAgent, BaseMultiActionAgent],\n",
    "        tools: Sequence[BaseTool],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AgentExecutor:\n",
    "        \"\"\"Create from agent and tools.\n",
    "\n",
    "        Args:\n",
    "            agent: Agent to use.\n",
    "            tools: Tools to use.\n",
    "            callbacks: Callbacks to use.\n",
    "            kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            AgentExecutor: Agent executor object.\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            agent=agent,\n",
    "            tools=tools,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @root_validator(pre=False, skip_on_failure=True)\n",
    "    def validate_tools(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that tools are compatible with agent.\n",
    "\n",
    "        Args:\n",
    "            values: Values to validate.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Validated values.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If allowed tools are different than provided tools.\n",
    "        \"\"\"\n",
    "        agent = values[\"agent\"]\n",
    "        tools = values[\"tools\"]\n",
    "        allowed_tools = agent.get_allowed_tools()\n",
    "        if allowed_tools is not None:\n",
    "            if set(allowed_tools) != set([tool.name for tool in tools]):\n",
    "                raise ValueError(\n",
    "                    f\"Allowed tools ({allowed_tools}) different than \"\n",
    "                    f\"provided tools ({[tool.name for tool in tools]})\"\n",
    "                )\n",
    "        return values\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_runnable_agent(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Convert runnable to agent if passed in.\n",
    "\n",
    "        Args:\n",
    "            values: Values to validate.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Validated values.\n",
    "        \"\"\"\n",
    "        agent = values.get(\"agent\")\n",
    "        if agent and isinstance(agent, Runnable):\n",
    "            try:\n",
    "                output_type = agent.OutputType\n",
    "            except Exception as _:\n",
    "                multi_action = False\n",
    "            else:\n",
    "                multi_action = output_type == Union[List[AgentAction], AgentFinish]\n",
    "\n",
    "            stream_runnable = values.pop(\"stream_runnable\", True)\n",
    "            if multi_action:\n",
    "                values[\"agent\"] = RunnableMultiActionAgent(\n",
    "                    runnable=agent, stream_runnable=stream_runnable\n",
    "                )\n",
    "            else:\n",
    "                values[\"agent\"] = RunnableAgent(\n",
    "                    runnable=agent, stream_runnable=stream_runnable\n",
    "                )\n",
    "        return values\n",
    "\n",
    "    def save(self, file_path: Union[Path, str]) -> None:\n",
    "        \"\"\"Raise error - saving not supported for Agent Executors.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to save to.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Saving not supported for agent executors.\n",
    "        \"\"\"\n",
    "        raise ValueError(\n",
    "            \"Saving not supported for agent executors. \"\n",
    "            \"If you are trying to save the agent, please use the \"\n",
    "            \"`.save_agent(...)`\"\n",
    "        )\n",
    "\n",
    "    def save_agent(self, file_path: Union[Path, str]) -> None:\n",
    "        \"\"\"Save the underlying agent.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to save to.\n",
    "        \"\"\"\n",
    "        return self.agent.save(file_path)\n",
    "\n",
    "    def iter(\n",
    "        self,\n",
    "        inputs: Any,\n",
    "        callbacks: Callbacks = None,\n",
    "        *,\n",
    "        include_run_info: bool = False,\n",
    "        async_: bool = False,  # arg kept for backwards compat, but ignored\n",
    "    ) -> AgentExecutorIterator:\n",
    "        \"\"\"Enables iteration over steps taken to reach final output.\n",
    "\n",
    "        Args:\n",
    "            inputs: Inputs to the agent.\n",
    "            callbacks: Callbacks to run.\n",
    "            include_run_info: Whether to include run info.\n",
    "            async_: Whether to run async. (Ignored)\n",
    "\n",
    "        Returns:\n",
    "            AgentExecutorIterator: Agent executor iterator object.\n",
    "        \"\"\"\n",
    "        return AgentExecutorIterator(\n",
    "            self,\n",
    "            inputs,\n",
    "            callbacks,\n",
    "            tags=self.tags,\n",
    "            include_run_info=include_run_info,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Return the input keys.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.agent.input_keys\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Return the singular output key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        if self.return_intermediate_steps:\n",
    "            return self.agent.return_values + [\"intermediate_steps\"]\n",
    "        else:\n",
    "            return self.agent.return_values\n",
    "\n",
    "    def lookup_tool(self, name: str) -> BaseTool:\n",
    "        \"\"\"Lookup tool by name.\n",
    "\n",
    "        Args:\n",
    "            name: Name of tool.\n",
    "\n",
    "        Returns:\n",
    "            BaseTool: Tool object.\n",
    "        \"\"\"\n",
    "        return {tool.name: tool for tool in self.tools}[name]\n",
    "\n",
    "    def _should_continue(self, iterations: int, time_elapsed: float) -> bool:\n",
    "        if self.max_iterations is not None and iterations >= self.max_iterations:\n",
    "            return False\n",
    "        if (\n",
    "            self.max_execution_time is not None\n",
    "            and time_elapsed >= self.max_execution_time\n",
    "        ):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _return(\n",
    "        self,\n",
    "        output: AgentFinish,\n",
    "        intermediate_steps: list,\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        if run_manager:\n",
    "            run_manager.on_agent_finish(output, color=\"green\", verbose=self.verbose)\n",
    "        final_output = output.return_values\n",
    "        if self.return_intermediate_steps:\n",
    "            final_output[\"intermediate_steps\"] = intermediate_steps\n",
    "        return final_output\n",
    "\n",
    "    async def _areturn(\n",
    "        self,\n",
    "        output: AgentFinish,\n",
    "        intermediate_steps: list,\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        if run_manager:\n",
    "            await run_manager.on_agent_finish(\n",
    "                output, color=\"green\", verbose=self.verbose\n",
    "            )\n",
    "        final_output = output.return_values\n",
    "        if self.return_intermediate_steps:\n",
    "            final_output[\"intermediate_steps\"] = intermediate_steps\n",
    "        return final_output\n",
    "\n",
    "    def _consume_next_step(\n",
    "        self, values: NextStepOutput\n",
    "    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n",
    "        if isinstance(values[-1], AgentFinish):\n",
    "            assert len(values) == 1\n",
    "            return values[-1]\n",
    "        else:\n",
    "            return [\n",
    "                (a.action, a.observation) for a in values if isinstance(a, AgentStep)\n",
    "            ]\n",
    "\n",
    "    def _take_next_step(\n",
    "        self,\n",
    "        name_to_tool_map: Dict[str, BaseTool],\n",
    "        color_mapping: Dict[str, str],\n",
    "        inputs: Dict[str, str],\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n",
    "        return self._consume_next_step(\n",
    "            [\n",
    "                a\n",
    "                for a in self._iter_next_step(\n",
    "                    name_to_tool_map,\n",
    "                    color_mapping,\n",
    "                    inputs,\n",
    "                    intermediate_steps,\n",
    "                    run_manager,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _iter_next_step(\n",
    "        self,\n",
    "        name_to_tool_map: Dict[str, BaseTool],\n",
    "        color_mapping: Dict[str, str],\n",
    "        inputs: Dict[str, str],\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Iterator[Union[AgentFinish, AgentAction, AgentStep]]:\n",
    "        \"\"\"Take a single step in the thought-action-observation loop.\n",
    "\n",
    "        Override this to take control of how the agent makes and acts on choices.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)\n",
    "\n",
    "            # Call the LLM to see what to do.\n",
    "            output = self.agent.plan(\n",
    "                intermediate_steps,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **inputs,\n",
    "            )\n",
    "        except OutputParserException as e:\n",
    "            if isinstance(self.handle_parsing_errors, bool):\n",
    "                raise_error = not self.handle_parsing_errors\n",
    "            else:\n",
    "                raise_error = False\n",
    "            if raise_error:\n",
    "                raise ValueError(\n",
    "                    \"An output parsing error occurred. \"\n",
    "                    \"In order to pass this error back to the agent and have it try \"\n",
    "                    \"again, pass `handle_parsing_errors=True` to the AgentExecutor. \"\n",
    "                    f\"This is the error: {str(e)}\"\n",
    "                )\n",
    "            text = str(e)\n",
    "            if isinstance(self.handle_parsing_errors, bool):\n",
    "                if e.send_to_llm:\n",
    "                    observation = str(e.observation)\n",
    "                    text = str(e.llm_output)\n",
    "                else:\n",
    "                    observation = \"Invalid or incomplete response\"\n",
    "            elif isinstance(self.handle_parsing_errors, str):\n",
    "                observation = self.handle_parsing_errors\n",
    "            elif callable(self.handle_parsing_errors):\n",
    "                observation = self.handle_parsing_errors(e)\n",
    "            else:\n",
    "                raise ValueError(\"Got unexpected type of `handle_parsing_errors`\")\n",
    "            output = AgentAction(\"_Exception\", observation, text)\n",
    "            if run_manager:\n",
    "                run_manager.on_agent_action(output, color=\"green\")\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            observation = ExceptionTool().run(\n",
    "                output.tool_input,\n",
    "                verbose=self.verbose,\n",
    "                color=None,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "            yield AgentStep(action=output, observation=observation)\n",
    "            return\n",
    "\n",
    "        # If the tool chosen is the finishing tool, then we end and return.\n",
    "        if isinstance(output, AgentFinish):\n",
    "            yield output\n",
    "            return\n",
    "\n",
    "        actions: List[AgentAction]\n",
    "        if isinstance(output, AgentAction):\n",
    "            actions = [output]\n",
    "        else:\n",
    "            actions = output\n",
    "        for agent_action in actions:\n",
    "            yield agent_action\n",
    "        for agent_action in actions:\n",
    "            yield self._perform_agent_action(\n",
    "                name_to_tool_map, color_mapping, agent_action, run_manager\n",
    "            )\n",
    "\n",
    "    def _perform_agent_action(\n",
    "        self,\n",
    "        name_to_tool_map: Dict[str, BaseTool],\n",
    "        color_mapping: Dict[str, str],\n",
    "        agent_action: AgentAction,\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> AgentStep:\n",
    "        if run_manager:\n",
    "            run_manager.on_agent_action(agent_action, color=\"green\")\n",
    "        # Otherwise we lookup the tool\n",
    "        if agent_action.tool in name_to_tool_map:\n",
    "            tool = name_to_tool_map[agent_action.tool]\n",
    "            return_direct = tool.return_direct\n",
    "            color = color_mapping[agent_action.tool]\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            if return_direct:\n",
    "                tool_run_kwargs[\"llm_prefix\"] = \"\"\n",
    "            # We then call the tool on the tool input to get an observation\n",
    "            observation = tool.run(\n",
    "                agent_action.tool_input,\n",
    "                verbose=self.verbose,\n",
    "                color=color,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            observation = InvalidTool().run(\n",
    "                {\n",
    "                    \"requested_tool_name\": agent_action.tool,\n",
    "                    \"available_tool_names\": list(name_to_tool_map.keys()),\n",
    "                },\n",
    "                verbose=self.verbose,\n",
    "                color=None,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        return AgentStep(action=agent_action, observation=observation)\n",
    "\n",
    "    async def _atake_next_step(\n",
    "        self,\n",
    "        name_to_tool_map: Dict[str, BaseTool],\n",
    "        color_mapping: Dict[str, str],\n",
    "        inputs: Dict[str, str],\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n",
    "        return self._consume_next_step(\n",
    "            [\n",
    "                a\n",
    "                async for a in self._aiter_next_step(\n",
    "                    name_to_tool_map,\n",
    "                    color_mapping,\n",
    "                    inputs,\n",
    "                    intermediate_steps,\n",
    "                    run_manager,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    async def _aiter_next_step(\n",
    "        self,\n",
    "        name_to_tool_map: Dict[str, BaseTool],\n",
    "        color_mapping: Dict[str, str],\n",
    "        inputs: Dict[str, str],\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> AsyncIterator[Union[AgentFinish, AgentAction, AgentStep]]:\n",
    "        \"\"\"Take a single step in the thought-action-observation loop.\n",
    "\n",
    "        Override this to take control of how the agent makes and acts on choices.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)\n",
    "\n",
    "            # Call the LLM to see what to do.\n",
    "            output = await self.agent.aplan(\n",
    "                intermediate_steps,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **inputs,\n",
    "            )\n",
    "        except OutputParserException as e:\n",
    "            if isinstance(self.handle_parsing_errors, bool):\n",
    "                raise_error = not self.handle_parsing_errors\n",
    "            else:\n",
    "                raise_error = False\n",
    "            if raise_error:\n",
    "                raise ValueError(\n",
    "                    \"An output parsing error occurred. \"\n",
    "                    \"In order to pass this error back to the agent and have it try \"\n",
    "                    \"again, pass `handle_parsing_errors=True` to the AgentExecutor. \"\n",
    "                    f\"This is the error: {str(e)}\"\n",
    "                )\n",
    "            text = str(e)\n",
    "            if isinstance(self.handle_parsing_errors, bool):\n",
    "                if e.send_to_llm:\n",
    "                    observation = str(e.observation)\n",
    "                    text = str(e.llm_output)\n",
    "                else:\n",
    "                    observation = \"Invalid or incomplete response\"\n",
    "            elif isinstance(self.handle_parsing_errors, str):\n",
    "                observation = self.handle_parsing_errors\n",
    "            elif callable(self.handle_parsing_errors):\n",
    "                observation = self.handle_parsing_errors(e)\n",
    "            else:\n",
    "                raise ValueError(\"Got unexpected type of `handle_parsing_errors`\")\n",
    "            output = AgentAction(\"_Exception\", observation, text)\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            observation = await ExceptionTool().arun(\n",
    "                output.tool_input,\n",
    "                verbose=self.verbose,\n",
    "                color=None,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "            yield AgentStep(action=output, observation=observation)\n",
    "            return\n",
    "\n",
    "        # If the tool chosen is the finishing tool, then we end and return.\n",
    "        if isinstance(output, AgentFinish):\n",
    "            yield output\n",
    "            return\n",
    "\n",
    "        actions: List[AgentAction]\n",
    "        if isinstance(output, AgentAction):\n",
    "            actions = [output]\n",
    "        else:\n",
    "            actions = output\n",
    "        for agent_action in actions:\n",
    "            yield agent_action\n",
    "\n",
    "        # Use asyncio.gather to run multiple tool.arun() calls concurrently\n",
    "        result = await asyncio.gather(\n",
    "            *[\n",
    "                self._aperform_agent_action(\n",
    "                    name_to_tool_map, color_mapping, agent_action, run_manager\n",
    "                )\n",
    "                for agent_action in actions\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # TODO This could yield each result as it becomes available\n",
    "        for chunk in result:\n",
    "            yield chunk\n",
    "\n",
    "    async def _aperform_agent_action(\n",
    "        self,\n",
    "        name_to_tool_map: Dict[str, BaseTool],\n",
    "        color_mapping: Dict[str, str],\n",
    "        agent_action: AgentAction,\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> AgentStep:\n",
    "        if run_manager:\n",
    "            await run_manager.on_agent_action(\n",
    "                agent_action, verbose=self.verbose, color=\"green\"\n",
    "            )\n",
    "        # Otherwise we lookup the tool\n",
    "        if agent_action.tool in name_to_tool_map:\n",
    "            tool = name_to_tool_map[agent_action.tool]\n",
    "            return_direct = tool.return_direct\n",
    "            color = color_mapping[agent_action.tool]\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            if return_direct:\n",
    "                tool_run_kwargs[\"llm_prefix\"] = \"\"\n",
    "            # We then call the tool on the tool input to get an observation\n",
    "            observation = await tool.arun(\n",
    "                agent_action.tool_input,\n",
    "                verbose=self.verbose,\n",
    "                color=color,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            observation = await InvalidTool().arun(\n",
    "                {\n",
    "                    \"requested_tool_name\": agent_action.tool,\n",
    "                    \"available_tool_names\": list(name_to_tool_map.keys()),\n",
    "                },\n",
    "                verbose=self.verbose,\n",
    "                color=None,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        return AgentStep(action=agent_action, observation=observation)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        # Construct a mapping of tool name to tool for easy lookup\n",
    "        name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "        # We construct a mapping from each tool to a color, used for logging.\n",
    "        color_mapping = get_color_mapping(\n",
    "            [tool.name for tool in self.tools], excluded_colors=[\"green\", \"red\"]\n",
    "        )\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "        # Let's start tracking the number of iterations and time elapsed\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # We now enter the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            next_step_output = self._take_next_step(\n",
    "                name_to_tool_map,\n",
    "                color_mapping,\n",
    "                inputs,\n",
    "                intermediate_steps,\n",
    "                run_manager=run_manager,\n",
    "            )\n",
    "            print(\"刚刚进入循环:\", next_step_output)\n",
    "            print(\"inputs:\", inputs)\n",
    "            \n",
    "            if isinstance(next_step_output, AgentFinish):\n",
    "                print('if isinstance(next_step_output, AgentFinish):', next_step_output)\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "\n",
    "            intermediate_steps.extend(next_step_output)\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                print(\"if len(next_step_output) == 1:\",next_step_action)\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                print(\"22222222222222222222222\", tool_return)\n",
    "                \n",
    "                if tool_return is not None:\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            \n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        \n",
    "        print(\"最后输出的内容:\", output)\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Async run text through and get agent response.\"\"\"\n",
    "        # Construct a mapping of tool name to tool for easy lookup\n",
    "        name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "        # We construct a mapping from each tool to a color, used for logging.\n",
    "        color_mapping = get_color_mapping(\n",
    "            [tool.name for tool in self.tools], excluded_colors=[\"green\"]\n",
    "        )\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "        # Let's start tracking the number of iterations and time elapsed\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # We now enter the agent loop (until it returns something).\n",
    "        try:\n",
    "            async with asyncio_timeout(self.max_execution_time):\n",
    "                while self._should_continue(iterations, time_elapsed):\n",
    "                    next_step_output = await self._atake_next_step(\n",
    "                        name_to_tool_map,\n",
    "                        color_mapping,\n",
    "                        inputs,\n",
    "                        intermediate_steps,\n",
    "                        run_manager=run_manager,\n",
    "                    )\n",
    "                    if isinstance(next_step_output, AgentFinish):\n",
    "                        return await self._areturn(\n",
    "                            next_step_output,\n",
    "                            intermediate_steps,\n",
    "                            run_manager=run_manager,\n",
    "                        )\n",
    "\n",
    "                    intermediate_steps.extend(next_step_output)\n",
    "                    if len(next_step_output) == 1:\n",
    "                        next_step_action = next_step_output[0]\n",
    "                        # See if tool should return directly\n",
    "                        tool_return = self._get_tool_return(next_step_action)\n",
    "                        if tool_return is not None:\n",
    "                            return await self._areturn(\n",
    "                                tool_return, intermediate_steps, run_manager=run_manager\n",
    "                            )\n",
    "\n",
    "                    iterations += 1\n",
    "                    time_elapsed = time.time() - start_time\n",
    "                output = self.agent.return_stopped_response(\n",
    "                    self.early_stopping_method, intermediate_steps, **inputs\n",
    "                )\n",
    "                return await self._areturn(\n",
    "                    output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "        except (TimeoutError, asyncio.TimeoutError):\n",
    "            # stop early when interrupted by the async timeout\n",
    "            output = self.agent.return_stopped_response(\n",
    "                self.early_stopping_method, intermediate_steps, **inputs\n",
    "            )\n",
    "            return await self._areturn(\n",
    "                output, intermediate_steps, run_manager=run_manager\n",
    "            )\n",
    "\n",
    "    def _get_tool_return(\n",
    "        self, next_step_output: Tuple[AgentAction, str]\n",
    "    ) -> Optional[AgentFinish]:\n",
    "        \"\"\"Check if the tool is a returning tool.\"\"\"\n",
    "        agent_action, observation = next_step_output\n",
    "        name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "        return_value_key = \"output\"\n",
    "        if len(self.agent.return_values) > 0:\n",
    "            return_value_key = self.agent.return_values[0]\n",
    "        # Invalid tools won't be in the map, so we return False.\n",
    "        if agent_action.tool in name_to_tool_map:\n",
    "            if name_to_tool_map[agent_action.tool].return_direct:\n",
    "                return AgentFinish(\n",
    "                    {return_value_key: observation},\n",
    "                    \"\",\n",
    "                )\n",
    "        return None\n",
    "\n",
    "    def _prepare_intermediate_steps(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]]\n",
    "    ) -> List[Tuple[AgentAction, str]]:\n",
    "        if (\n",
    "            isinstance(self.trim_intermediate_steps, int)\n",
    "            and self.trim_intermediate_steps > 0\n",
    "        ):\n",
    "            return intermediate_steps[-self.trim_intermediate_steps :]\n",
    "        elif callable(self.trim_intermediate_steps):\n",
    "            return self.trim_intermediate_steps(intermediate_steps)\n",
    "        else:\n",
    "            return intermediate_steps\n",
    "\n",
    "    def stream(\n",
    "        self,\n",
    "        input: Union[Dict[str, Any], Any],\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[AddableDict]:\n",
    "        \"\"\"Enables streaming over steps taken to reach final output.\n",
    "\n",
    "        Args:\n",
    "            input: Input to the agent.\n",
    "            config: Config to use.\n",
    "            kwargs: Additional arguments.\n",
    "\n",
    "        Yields:\n",
    "            AddableDict: Addable dictionary.\n",
    "        \"\"\"\n",
    "        config = ensure_config(config)\n",
    "        iterator = AgentExecutorIterator(\n",
    "            self,\n",
    "            input,\n",
    "            config.get(\"callbacks\"),\n",
    "            tags=config.get(\"tags\"),\n",
    "            metadata=config.get(\"metadata\"),\n",
    "            run_name=config.get(\"run_name\"),\n",
    "            run_id=config.get(\"run_id\"),\n",
    "            yield_actions=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for step in iterator:\n",
    "            yield step\n",
    "\n",
    "    async def astream(\n",
    "        self,\n",
    "        input: Union[Dict[str, Any], Any],\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[AddableDict]:\n",
    "        \"\"\"Async enables streaming over steps taken to reach final output.\n",
    "\n",
    "        Args:\n",
    "            input: Input to the agent.\n",
    "            config: Config to use.\n",
    "            kwargs: Additional arguments.\n",
    "\n",
    "        Yields:\n",
    "            AddableDict: Addable dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        config = ensure_config(config)\n",
    "        iterator = AgentExecutorIterator(\n",
    "            self,\n",
    "            input,\n",
    "            config.get(\"callbacks\"),\n",
    "            tags=config.get(\"tags\"),\n",
    "            metadata=config.get(\"metadata\"),\n",
    "            run_name=config.get(\"run_name\"),\n",
    "            run_id=config.get(\"run_id\"),\n",
    "            yield_actions=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        async for step in iterator:\n",
    "            yield step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 你好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\"], llm)  # 移除了serpapi，保留llm-math工具\n",
    "tools.append(create_react_agent_demo)\n",
    "\n",
    "agent = create_react_agent(\n",
    "  llm=llm,\n",
    "  tools=tools,\n",
    "  prompt=prompt,\n",
    ")\n",
    "agent_executor = AgentExecutor(\n",
    "  llm=llm,\n",
    "  agent=agent, \n",
    "  tools=tools, \n",
    "  handle_parsing_errors=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刚刚进入循环: [(AgentAction(tool='create_react_agent_demo', tool_input='美国当前总统', log='这个问题需要查找最新的美国总统信息。我将使用create_react_agent_demo工具来搜索。\\nAction: create_react_agent_demo\\nAction Input: \"美国当前总统\"'), None)]\n",
      "inputs: {'input': '美国总统是谁'}\n",
      "if len(next_step_output) == 1: (AgentAction(tool='create_react_agent_demo', tool_input='美国当前总统', log='这个问题需要查找最新的美国总统信息。我将使用create_react_agent_demo工具来搜索。\\nAction: create_react_agent_demo\\nAction Input: \"美国当前总统\"'), None)\n",
      "22222222222222222222222 None\n",
      "刚刚进入循环: return_values={'output': '请稍等，我将为您提供美国当前的总统信息。'} log='我需要等待工具返回结果以获取美国总统的信息。在收到结果后，我将提供最终答案。\\nFinal Answer: 请稍等，我将为您提供美国当前的总统信息。'\n",
      "inputs: {'input': '美国总统是谁'}\n",
      "if isinstance(next_step_output, AgentFinish): return_values={'output': '请稍等，我将为您提供美国当前的总统信息。'} log='我需要等待工具返回结果以获取美国总统的信息。在收到结果后，我将提供最终答案。\\nFinal Answer: 请稍等，我将为您提供美国当前的总统信息。'\n"
     ]
    }
   ],
   "source": [
    "s=agent_executor._call({\"input\": \"美国总统是谁\"})\n",
    "\n",
    "# # Use with chat history\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "# agent_executor.invoke(\n",
    "#     {\n",
    "#         \"input\": \"what's my name?\",\n",
    "#         # Notice that chat_history is a string\n",
    "#         # since this prompt is aimed at LLMs, not chat models\n",
    "#         \"chat_history\": \"Human: My name is Bob\\nAI: Hello Bob!\",\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '请稍等，我将为您提供美国当前的总统信息。'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = load_tools([\"llm-math\"], llm)  # 移除了serpapi，保留llm-math工具\n",
    "# tools.append(create_react_agent_demo)\n",
    "# ljh_demo = ljh()\n",
    "\n",
    "# agent = ljh_demo.create_react_agent(\n",
    "#   llm=llm,\n",
    "#   tools=tools,\n",
    "#   prompt=prompt,\n",
    "# )\n",
    "# agent_executor = AgentExecutor(\n",
    "#   agent=agent, \n",
    "#   tools=tools, \n",
    "#   handle_parsing_errors=True,\n",
    "#   max_execution_time=2\n",
    "#   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
