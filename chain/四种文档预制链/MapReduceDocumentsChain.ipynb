{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 源码分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapReduceDocumentsChain(BaseCombineDocumentsChain):\n",
    "    \"\"\"Combining documents by mapping a chain over them, then combining results.\n",
    "\n",
    "    We first call `llm_chain` on each document individually, passing in the\n",
    "    `page_content` and any other kwargs. This is the `map` step.\n",
    "\n",
    "    We then process the results of that `map` step in a `reduce` step. This should\n",
    "    likely be a ReduceDocumentsChain.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MapReduceDocumentsChain` 是一种处理文档的链式操作类，首先将 `llm_chain` 应用于每个文档（`map` 步骤），然后在 `reduce` 步骤中合并这些结果。该类继承自 `BaseCombineDocumentsChain`，用于实现文档的 `map-reduce` 处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `llm_chain: LLMChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain: LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明**：这是应用于每个文档的链，通常是一个语言模型链（LLMChain）。\n",
    "- **作用**：在 `map` 步骤中，对每个文档单独调用 `llm_chain`，生成初步结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `reduce_documents_chain: BaseCombineDocumentsChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_documents_chain: BaseCombineDocumentsChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明**：用于合并 `map` 步骤生成的结果。\n",
    "- **作用**：在 `reduce` 步骤中，调用该链合并 `map` 步骤生成的结果，生成最终输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `document_variable_name: str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_variable_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明**：指定传递给 `llm_chain` 的文档变量名。\n",
    "- **作用**：在处理文档时，将文档内容映射到这个变量名上，以便传递给 LLMChain。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `return_intermediate_steps: bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_intermediate_steps: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明**：是否返回中间步骤的结果。\n",
    "- **作用**：如果设置为 `True`，在最终输出中会包含 `map` 步骤生成的中间结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_output_schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_schema(self, config: Optional[RunnableConfig] = None) -> Type[BaseModel]:\n",
    "    if self.return_intermediate_steps:\n",
    "        return create_model(\n",
    "            \"MapReduceDocumentsOutput\",\n",
    "            **{\n",
    "                self.output_key: (str, None),\n",
    "                \"intermediate_steps\": (List[str], None),\n",
    "            },\n",
    "        )\n",
    "    return super().get_output_schema(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：定义输出的模式。\n",
    "- 作用：如果 `return_intermediate_steps` 为 `True`，该方法会返回包含中间步骤的输出模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `output_keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def output_keys(self) -> List[str]:\n",
    "    \"\"\"Expect input key.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "    _output_keys = super().output_keys\n",
    "    if self.return_intermediate_steps:\n",
    "        _output_keys = _output_keys + [\"intermediate_steps\"]\n",
    "    return _output_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：返回期望的输出键。\n",
    "- 作用：根据 `return_intermediate_steps` 的设置，决定是否包含中间步骤的输出键。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_reduce_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@root_validator(pre=True)\n",
    "def get_reduce_chain(cls, values: Dict) -> Dict:\n",
    "    \"\"\"For backwards compatibility.\"\"\"\n",
    "    if \"combine_document_chain\" in values:\n",
    "        if \"reduce_documents_chain\" in values:\n",
    "            raise ValueError(\n",
    "                \"Both `reduce_documents_chain` and `combine_document_chain` \"\n",
    "                \"cannot be provided at the same time. `combine_document_chain` \"\n",
    "                \"is deprecated, please only provide `reduce_documents_chain`\"\n",
    "            )\n",
    "        combine_chain = values[\"combine_document_chain\"]\n",
    "        collapse_chain = values.get(\"collapse_document_chain\")\n",
    "        reduce_chain = ReduceDocumentsChain(\n",
    "            combine_documents_chain=combine_chain,\n",
    "            collapse_documents_chain=collapse_chain,\n",
    "        )\n",
    "        values[\"reduce_documents_chain\"] = reduce_chain\n",
    "        del values[\"combine_document_chain\"]\n",
    "        if \"collapse_document_chain\" in values:\n",
    "            del values[\"collapse_document_chain\"]\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：在使用时兼容旧的 `combine_document_chain` 参数。\n",
    "- 作用：确保向后兼容性，将旧的 `combine_document_chain` 转换为新的 `reduce_documents_chain`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_return_intermediate_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@root_validator(pre=True)\n",
    "def get_return_intermediate_steps(cls, values: Dict) -> Dict:\n",
    "    \"\"\"For backwards compatibility.\"\"\"\n",
    "    if \"return_map_steps\" in values:\n",
    "        values[\"return_intermediate_steps\"] = values[\"return_map_steps\"]\n",
    "        del values[\"return_map_steps\"]\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：处理向后兼容性，支持旧的 `return_map_steps` 参数。\n",
    "- 作用：将旧的 `return_map_steps` 参数转换为新的 `return_intermediate_steps`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_default_document_variable_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@root_validator(pre=True)\n",
    "def get_default_document_variable_name(cls, values: Dict) -> Dict:\n",
    "    \"\"\"Get default document variable name, if not provided.\"\"\"\n",
    "    if \"llm_chain\" not in values:\n",
    "        raise ValueError(\"llm_chain must be provided\")\n",
    "\n",
    "    llm_chain_variables = values[\"llm_chain\"].prompt.input_variables\n",
    "    if \"document_variable_name\" not in values:\n",
    "        if len(llm_chain_variables) == 1:\n",
    "            values[\"document_variable_name\"] = llm_chain_variables[0]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"document_variable_name must be provided if there are \"\n",
    "                \"multiple llm_chain input_variables\"\n",
    "            )\n",
    "    else:\n",
    "        if values[\"document_variable_name\"] not in llm_chain_variables:\n",
    "            raise ValueError(\n",
    "                f\"document_variable_name {values['document_variable_name']} was \"\n",
    "                f\"not found in llm_chain input_variables: {llm_chain_variables}\"\n",
    "            )\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：如果没有提供 `document_variable_name`，则推断默认值。\n",
    "- 作用：根据 `llm_chain` 的提示模板自动推断出文档变量名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `combine_docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(\n",
    "    self,\n",
    "    docs: List[Document],\n",
    "    token_max: Optional[int] = None,\n",
    "    callbacks: Callbacks = None,\n",
    "    **kwargs: Any,\n",
    ") -> Tuple[str, dict]:\n",
    "    \"\"\"Combine documents in a map reduce manner.\n",
    "\n",
    "    Combine by mapping first chain over all documents, then reducing the results.\n",
    "    This reducing can be done recursively if needed (if there are many documents).\n",
    "    \"\"\"\n",
    "    map_results = self.llm_chain.apply(\n",
    "        # FYI - this is parallelized and so it is fast.\n",
    "        [{self.document_variable_name: d.page_content, **kwargs} for d in docs],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    question_result_key = self.llm_chain.output_key\n",
    "    result_docs = [\n",
    "        Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n",
    "        # This uses metadata from the docs, and the textual results from `results`\n",
    "        for i, r in enumerate(map_results)\n",
    "    ]\n",
    "    result, extra_return_dict = self.reduce_documents_chain.combine_docs(\n",
    "        result_docs, token_max=token_max, callbacks=callbacks, **kwargs\n",
    "    )\n",
    "    if self.return_intermediate_steps:\n",
    "        intermediate_steps = [r[question_result_key] for r in map_results]\n",
    "        extra_return_dict[\"intermediate_steps\"] = intermediate_steps\n",
    "    return result, extra_return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：按 `map-reduce` 的方式合并文档。\n",
    "- 作用：\n",
    "  - `map` 步骤：对每个文档调用 `llm_chain` 生成初步结果。\n",
    "  - `reduce` 步骤：将初步结果传递给 `reduce_documents_chain` 合并生成最终输出。\n",
    "  - 可选地返回中间步骤的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `acombine_docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def acombine_docs(\n",
    "    self,\n",
    "    docs: List[Document],\n",
    "    token_max: Optional[int] = None,\n",
    "    callbacks: Callbacks = None,\n",
    "    **kwargs: Any,\n",
    ") -> Tuple[str, dict]:\n",
    "    \"\"\"Async combine documents in a map reduce manner.\n",
    "\n",
    "    Combine by mapping first chain over all documents, then reducing the results.\n",
    "    This reducing can be done recursively if needed (if there are many documents).\n",
    "    \"\"\"\n",
    "    map_results = await self.llm_chain.aapply(\n",
    "        # FYI - this is parallelized and so it is fast.\n",
    "        [{**{self.document_variable_name: d.page_content}, **kwargs} for d in docs],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    question_result_key = self.llm_chain.output_key\n",
    "    result_docs = [\n",
    "        Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n",
    "        # This uses metadata from the docs, and the textual results from `results`\n",
    "        for i, r in enumerate(map_results)\n",
    "    ]\n",
    "    result, extra_return_dict = await self.reduce_documents_chain.acombine_docs(\n",
    "        result_docs, token_max=token_max, callbacks=callbacks, **kwargs\n",
    "    )\n",
    "    if self.return_intermediate_steps:\n",
    "        intermediate_steps = [r[question_result_key] for r in map_results]\n",
    "        extra_return_dict[\"intermediate_steps\"] = intermediate_steps\n",
    "    return result, extra_return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：`combine_docs` 的异步版本。\n",
    "- 作用：\n",
    "  - 以异步方式执行 `map-reduce` 处理。\n",
    "  - 使用 `map` 步骤生成初步结果，并将其传递给 `reduce_documents_chain` 进行最终合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `_chain_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def _chain_type(self) -> str:\n",
    "    return \"map_reduce_documents_chain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：返回链类型。\n",
    "- 作用：标识此链的类型为 `\"map_reduce_documents_chain\"`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个 `MapReduceDocumentsChain` 类是一个强大的工具，可以将复杂的文档处理工作分解成多个步骤，通过并行和递归的方式高效地处理大量文档，并最终生成一个合并的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.combine_documents.reduce import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain_core.documents.base import Document\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 从环境变量中获取API密钥，用于初始化 ChatTongyi 模型\n",
    "api_key = os.getenv(\"KEY_TONGYI\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key is not set. Please ensure that the 'KEY_TONGYI' environment variable is set.\")\n",
    "\n",
    "\n",
    "# 初始化 ChatTongyi 模型，设置文本生成的温度参数，温度越低生成的文本越接近输入\n",
    "llm = ChatTongyi(\n",
    "    dashscope_api_key=api_key,\n",
    "    temperature=0,  # 设置生成文本的倾向，值越小生成的文本越接近输入\n",
    "    streaming=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_prompt = PromptTemplate(\n",
    "  input_variables=[\"page_content\"],\n",
    "  template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=document_prompt,\n",
    ")\n",
    "reduce_prompt = PromptTemplate.from_template(\"总结这些评论:{context}\")\n",
    "\n",
    "reduce_llm_chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=reduce_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于将文档内容插入到 reduce_llm_chain 中，并生成合并的输出。\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "  llm_chain=reduce_llm_chain,\n",
    "  document_prompt=document_prompt,\n",
    "  document_variable_name=\"context\"\n",
    ")\n",
    "# 是用于将 map 阶段生成的多个结果合并在一起的链.\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MapReduceDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 准备一组文档\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain 是一个强大的库，帮助你轻松处理文档。\"),\n",
    "    Document(page_content=\"OpenAI 的 GPT-3 模型提供了强大的文本生成能力。\"),\n",
    "    Document(page_content=\"通过组合这些工具，你可以实现强大的 NLP 应用。\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 执行链，生成最终总结\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:237\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    232\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    233\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    236\u001b[0m ]\n\u001b[1;32m--> 237\u001b[0m result, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    238\u001b[0m     result_docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    239\u001b[0m )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_intermediate_steps:\n\u001b[0;32m    241\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m [r[question_result_key] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m map_results]\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:240\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    220\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    224\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     result_docs, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse(\n\u001b[0;32m    241\u001b[0m         docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    242\u001b[0m     )\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    244\u001b[0m         docs\u001b[38;5;241m=\u001b[39mresult_docs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    245\u001b[0m     )\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:285\u001b[0m, in \u001b[0;36mReduceDocumentsChain._collapse\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m docs\n\u001b[0;32m    284\u001b[0m length_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mprompt_length\n\u001b[1;32m--> 285\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m length_func(result_docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collapse_docs_func\u001b[39m(docs: List[Document], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    289\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    290\u001b[0m     )\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:239\u001b[0m, in \u001b[0;36mStuffDocumentsChain.prompt_length\u001b[1;34m(self, docs, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain\\chains\\llm.py:407\u001b[0m, in \u001b[0;36mLLMChain._get_num_tokens\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_language_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain_core\\language_models\\base.py:346\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        The integer number of tokens in the text.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain_core\\language_models\\base.py:333\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_token_ids\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_get_token_ids(text)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain_core\\language_models\\base.py:62\u001b[0m, in \u001b[0;36m_get_token_ids_default_method\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain_core\\language_models\\base.py:56\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPT2TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2255\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2258\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2259\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m     )\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "# 执行链，生成最终总结\n",
    "result, _ = chain.combine_docs(docs)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
