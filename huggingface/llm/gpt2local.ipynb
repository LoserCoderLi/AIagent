{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "e:\\AAAAWork\\python\\LLM_RAG\\demo\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step. First of all, we do not know how to do it. We do not know what frequencies and distances we can obtain. However, a person with visual awareness can have all kinds of information in a digital computer. So let's say you look at a website and you type in data with your brain. How can you use one or more of these signals to see the world? A laser could really help you. But, at a technical level, it is far from conclusive.\n",
      "\n",
      "Let's say you go to college and you say that you want to study electrical engineering. In order to enroll at an electric engineering school, you have to be able to get a computer that is able to read the computer signals and then you can see the signs there, like the distance between the computer and their walls. Some of them, they see as far from the wall as you can see from the computer's screen. And so on. These is also pretty far from the wall.\n",
      "\n",
      "But at\n"
     ]
    }
   ],
   "source": [
    "#加载依赖\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "#加载本地模型\n",
    "#model_id = \"G:\\hugging_fase_model2\\gemma-7b\"\n",
    "model_id = r\"E:\\AAAAWork\\python\\models\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# 增加 max_new_tokens 的值以生成更长的文本\n",
    "max_new_tokens = 200  # 可以根据需要调整这个值\n",
    "\n",
    "#构建管道\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "#构建提示词模版\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "#LCEL\n",
    "chain = prompt | hf\n",
    "\n",
    "#提问\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "#输出\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
